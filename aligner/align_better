#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
import random

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Training with EM...")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]
f_words = set()
e_words = set()
epochs = 10

# uniformly set transitional probabilities
for (n, (f, e)) in enumerate(bitext):    
  for e_word in e:
    e_words.add(e_word)
  for f_word in f:
    f_words.add(f_word)

ef_trans_prob = defaultdict(lambda: random.uniform(0,1))
q = defaultdict(lambda: random.uniform(0,1))

for epoch in range(epochs):
  # intitialize
  total_f = defaultdict(float)
  ef_count = defaultdict(float)
  
  for (n, (f, e)) in enumerate(bitext):
    # compute normalization
    s_total = defaultdict(float)

    for e_word in e:
      for f_word in f:
        s_total[e_word] += ef_trans_prob[(e_word,f_word)]

    # collect counts
    for e_word in e:
      for f_word in f:
        ef_count[(e_word,f_word)] += ef_trans_prob[(e_word,f_word)]/s_total[e_word]
        total_f[f_word] += ef_trans_prob[(e_word,f_word)]/s_total[e_word]

  # estimate probabilities 
  for (e_word,f_word) in ef_count:
    ef_trans_prob[(e_word,f_word)] = ef_count[(e_word,f_word)]/total_f[f_word]
"""
# calculate alignment probabilities
for (k, (f, e)) in enumerate(bitext):
  m = len(e)
  l = len(f)
  c1 = defaultdict(float) # c(j|i,l,m)
  c2 = defaultdict(float) # c(i,l,m)

  for i in range(1,m): # len of e
    for j in range(l): # len of f
      qsum = 0
      for jsum in range(l):
        qsum += q[(jsum,i,l,m)]*ef_trans_prob[(e[i],f[jsum])]

      delta = q[(j,i,l,m)]*ef_trans_prob[(e[i],f[j])]/qsum

      c1[(i,j,l,m)] += delta
      c2[(j,l,m)] += delta

  for (i,j,ll,mm) in c1:
    q[(i,j,ll,mm)] += c1[(i,j,ll,mm)]/c2[(j,ll,mm)]
"""
for (f, e) in bitext:
  for (i, f_word) in enumerate(f): 
    for (j, e_word) in enumerate(e):
      # if ef_trans_prob[(e_word,f_word)] * q[(i,j,len(f),len(e))] >= opts.threshold:
      if ef_trans_prob[(e_word,f_word)] >= opts.threshold:
        sys.stdout.write("%i-%i " % (i,j))
  sys.stdout.write("\n")
